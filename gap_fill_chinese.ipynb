{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ed0f0b-280d-4488-a70e-7b5f271e1985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9192,\n",
       " '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='./ChnSentiCorp', split=split)\n",
    "\n",
    "        def f(data):\n",
    "            return len(data['text']) > 30\n",
    "\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c52a84-354f-410f-bb25-02dfe0c8c802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./model_dir/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('./model_dir/bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29459b3e-9c11-4f07-a6ef-12224b218ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n",
      "[CLS] 外 观 漂 亮 ， 最 大 的 卖 点 。 键 盘 大 [MASK] 联 想 ｓ１０ 被 淘 汰 的 原 因 。 电 池 [SEP]\n",
      "，\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 30]),\n",
       " torch.Size([16, 30]),\n",
       " torch.Size([16, 30]),\n",
       " torch.Size([16]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=data,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=30,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    #把第15个词固定替换为mask\n",
    "    labels = input_ids[:, 15].reshape(-1).clone()\n",
    "    input_ids[:, 15] = token.get_vocab()[token.mask_token]\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(token.decode(input_ids[0]))\n",
    "print(token.decode(labels[0]))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ff3c01-18dd-4683-981c-f6962b30051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device is: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"the device is: {device}\")\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('./model_dir/bert-base-chinese').to(device)\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "token_type_ids = token_type_ids.to(device)\n",
    "# labels = labels.to(device)\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0df3f481-6862-49b3-8fbe-e1d696dce9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token vocab_size: 21128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 21128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "print(f\"token vocab_size: {token.vocab_size}\")\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = torch.nn.Linear(768, token.vocab_size, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.decoder(out.last_hidden_state[:, 15])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1f99f8d-c9e7-4bb2-b87a-c8e8b8126d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user_home/xiaomingxu/mambaforge/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 10.133556365966797 0.0\n",
      "0 50 8.386566162109375 0.125\n",
      "0 100 5.440138339996338 0.1875\n",
      "0 150 5.240077018737793 0.3125\n",
      "0 200 4.803647041320801 0.3125\n",
      "0 250 4.083328723907471 0.625\n",
      "0 300 5.012510299682617 0.375\n",
      "0 350 4.216627597808838 0.375\n",
      "0 400 3.810495376586914 0.375\n",
      "0 450 4.103509426116943 0.4375\n",
      "0 500 3.2732534408569336 0.625\n",
      "0 550 3.49190616607666 0.5625\n",
      "1 0 1.9476391077041626 0.6875\n",
      "1 50 2.148423910140991 0.625\n",
      "1 100 2.345219373703003 0.6875\n",
      "1 150 2.120758533477783 0.75\n",
      "1 200 2.350722312927246 0.625\n",
      "1 250 2.3131985664367676 0.5625\n",
      "1 300 1.674559235572815 0.8125\n",
      "1 350 2.7869696617126465 0.5\n",
      "1 400 3.904294013977051 0.3125\n",
      "1 450 3.5059802532196045 0.4375\n",
      "1 500 1.6510206460952759 0.6875\n",
      "1 550 1.0694546699523926 0.875\n",
      "2 0 0.8782647252082825 0.8125\n",
      "2 50 1.0888144969940186 0.8125\n",
      "2 100 0.6853850483894348 0.875\n",
      "2 150 1.3918399810791016 0.6875\n",
      "2 200 1.5589661598205566 0.75\n",
      "2 250 0.5652564764022827 0.8125\n",
      "2 300 1.232049822807312 0.75\n",
      "2 350 0.5137938857078552 0.9375\n",
      "2 400 0.7003031373023987 1.0\n",
      "2 450 0.34572461247444153 1.0\n",
      "2 500 1.1333558559417725 0.8125\n",
      "2 550 0.8796539306640625 0.875\n",
      "3 0 0.1632440984249115 1.0\n",
      "3 50 0.38094139099121094 0.9375\n",
      "3 100 0.8391824960708618 0.8125\n",
      "3 150 0.36620527505874634 1.0\n",
      "3 200 0.9398525357246399 0.75\n",
      "3 250 0.44771480560302734 0.9375\n",
      "3 300 0.30156776309013367 0.9375\n",
      "3 350 0.5026407837867737 0.875\n",
      "3 400 0.5587416291236877 0.8125\n",
      "3 450 0.8480291962623596 0.8125\n",
      "3 500 0.5396453142166138 0.9375\n",
      "3 550 0.9652802348136902 0.8125\n",
      "4 0 0.3817703425884247 0.9375\n",
      "4 50 0.20763273537158966 1.0\n",
      "4 100 0.5534794926643372 0.875\n",
      "4 150 0.4493538439273834 0.9375\n",
      "4 200 0.366155207157135 0.9375\n",
      "4 250 0.3768833875656128 0.9375\n",
      "4 300 0.5601391196250916 0.8125\n",
      "4 350 0.1458451747894287 1.0\n",
      "4 400 0.49700284004211426 0.875\n",
      "4 450 0.5298435688018799 0.8125\n",
      "4 500 0.3769778609275818 0.875\n",
      "4 550 0.5393630266189575 0.875\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "            print(epoch, i, loss.item(), accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ebd07d1-75bc-4433-9708-c719da626d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518296/1108353750.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[CLS] 该 酒 点 实 在 太 差, 携 程 非 常 不 负 [MASK], 我 花 308 住 豪 华 房, 性 价 比 也 [SEP]\n",
      "心 责\n",
      "1\n",
      "[CLS] 一 直 是 再 重 复 说 男 人 要 得 是 成 绩 [MASK] 美, 女 人 要 得 是 聆 听 观 点 有 点 [SEP]\n",
      "赞 赞\n",
      "2\n",
      "[CLS] 6 月 30 日 入 住 的 。 房 间 总 体 还 行 [MASK] 就 是 有 点 旧 。 周 围 环 境 较 好 。 [SEP]\n",
      "。 ，\n",
      "3\n",
      "[CLS] 一 年 前 我 们 给 孩 子 6 岁 的 儿 子 买 [MASK] 卡 梅 拉 的 第 一 部 ， 孩 子 非 常 喜 [SEP]\n",
      "了 了\n",
      "4\n",
      "[CLS] 非 常 一 般 的 一 本 书 ， 充 满 了 假 想 [MASK] 理 想 主 义 色 彩 ， 建 议 刚 毕 业 的 [SEP]\n",
      "的 的\n",
      "5\n",
      "[CLS] 内 容 还 算 过 的 去 ， 不 过 以 文 采 来 [MASK] ， 就 一 般 。 说 的 只 是 一 些 大 道 [SEP]\n",
      "说 说\n",
      "6\n",
      "[CLS] 位 于 西 环 ， 地 处 香 港 老 城 区 ， 门 [MASK] 有 巴 士 及 电 车 站 ， 交 通 比 较 便 [SEP]\n",
      "口 口\n",
      "7\n",
      "[CLS] 三 个 usb 接 口 居 然 都 在 左 边 ， 接 鼠 [MASK] 很 不 方 便 ， 不 理 解 设 计 师 的 理 [SEP]\n",
      "盘 标\n",
      "8\n",
      "[CLS] 键 盘 太 拥 挤 按 着 不 太 舒 服, 也 容 [MASK] 按 错 键. 不 过 这 体 积 大 概 也 只 [SEP]\n",
      "易 易\n",
      "9\n",
      "[CLS] 我 于 6 月 1 日 再 次 入 住, 住 的 是 [MASK]2 房, 首 先 价 格 由 238 元 涨 到 278 [SEP]\n",
      "1 1 3 1\n",
      "10\n",
      "[CLS] 机 器 外 观 很 不 错 ， 干 净 ， 整 洁 ， [MASK] 感 很 好 。 完 美 屏 ， 音 响 效 果 相 [SEP]\n",
      "手 手\n",
      "11\n",
      "[CLS] 不 错 的 东 西 ， 拿 回 来 ， 第 一 感 觉 [MASK] 是 好 东 西 ！ 包 装 正 规 ， 没 有 拆 [SEP]\n",
      "就 就\n",
      "12\n",
      "[CLS] 这 次 入 住 的 是 大 床 房 ， 房 间 设 施 [MASK] 算 可 以 ， 没 想 象 中 那 么 旧 。 浴 [SEP]\n",
      "还 还\n",
      "13\n",
      "[CLS] 不 知 道 为 什 么 。 我 家 女 儿 就 是 不 [MASK] 欢 这 套 书 。 我 们 一 起 看 的 时 候 [SEP]\n",
      "喜 喜\n",
      "14\n",
      "[CLS] 大 俗 即 大 雅 ！ 这 是 看 郑 振 铎 先 生 [MASK] 部 书 后 最 由 衷 的 感 想 ， 看 过 中 [SEP]\n",
      "这 三\n",
      "0.6854166666666667\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained(\"./fine_tune_bert_model\")\n",
    "# token.save_pretrained(\"./fine_tune_bert_token\")\n",
    "# model.save(model.state_dict(), \"model_weights.path\")\n",
    "#torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "model = Model().to(device)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "# model = torch.load(\"model_weights.pth\")\n",
    "# model.to(device)\n",
    "\n",
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 15:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        print(token.decode(input_ids[0]))\n",
    "        print(token.decode(out[0]), token.decode(labels[0]))\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efacf7-56cf-4b06-a870-c818996a92dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
